{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import math\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration(object):\n",
    "    def __init__(self, config_path):\n",
    "        self.d_model= 256\n",
    "        self.FF_innerlayer_dim= 512\n",
    "        self.key_vector_dim= 128\n",
    "        self.value_vector_dim= 128\n",
    "        self.emb_dimension= 256\n",
    "        self.encoder_layer_num= 3\n",
    "        self.decoder_layer_num= 4\n",
    "        self.attention_num_heads= 3\n",
    "        self.batch_size = 32\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Configuration('./config.yaml')\n",
    "config.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformer <br>\n",
    "### - Attention Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    [Multi Head Self Attentional Module]\n",
    "     Where B= batch_size, S= sequence_size, D= model_dimeision, H= head_num\n",
    "     - Input= (B, S, D)\n",
    "     - Q and K vector= (B, S, d_k*H) / V vector= (B, S, d_v*H)\n",
    "     - Attention= (B, H, S, S)\n",
    "     - Context= (B, H, S, d_v)\n",
    "     - Output= (B, S, D) \n",
    "\"\"\"\n",
    "class Multihead_SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.batch_size = config.batch_size\n",
    "        self.d_k = config.key_vector_dim\n",
    "        self.d_v = config.value_vector_dim\n",
    "        self.h = config.attention_num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(self.d_model, self.d_k*self.h)\n",
    "        self.W_k = nn.Linear(self.d_model, self.d_k*self.h)\n",
    "        self.W_v = nn.Linear(self.d_model, self.d_v*self.h)\n",
    "        self.W_o = nn.Linear(self.d_v*self.h, self.d_model)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        # (OPTIONAL) for visualization. TODO: will be deleted later\n",
    "        # self.scores = None\n",
    "        \n",
    "    def scaled_dot_product(self, q, k, v, attn_mask):\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(k.size(-1))\n",
    "        \n",
    "        if attn_mask is not None: scores = scores.masked_fill(attn_mask, -1e9)\n",
    "        \n",
    "        attention = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attention, v)\n",
    "        return context, attention\n",
    "        \n",
    "    \n",
    "    def forward(self, q, k, v, attn_mask):\n",
    "        residual = q\n",
    "        q, k, v= self.W_q(q), self.W_k(k), self.W_v(v)\n",
    "        # (1) Split q, k, v vectors from (B, S, k(v)_dim*H) to (B, H, S, k(v)_dim) and create the corresponding attention masks\n",
    "        # q_s = (B, H, S, k_dim)\n",
    "        # k_s = (B, H, S, k_dim)\n",
    "        # v_s = (B, H, S, v_dim))\n",
    "        # attn_mask : (B, H, d_k, d_v)\n",
    "        q_s = q.view(self.batch_size, -1, self.h, self.d_k).transpose(1,2)\n",
    "        k_s = k.view(self.batch_size, -1, self.h, self.d_k).transpose(1,2)\n",
    "        v_s = v.view(self.batch_size, -1, self.h, self.d_v).transpose(1,2)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.h, 1, 1)\n",
    "        \n",
    "        # (2) Head_i = Scaled_Dot_Product(q*W^q_i, k*W^k_i, v*W^v_i)\n",
    "        context, attention = self.scaled_dot_product(q_s, k_s, v_s, attn_mask)\n",
    "        \n",
    "        # (3) Concat context vectors and Resize by using the W_o(Output weight)\n",
    "        concated = context.transpose(1, 2).contiguous().view(self.batch_size, -1, self.h*self.d_v)\n",
    "        output = self.W_o(concated)\n",
    "        return nn.LayerNorm(self.d_model)(output + residual), attention\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "[Position-wise Feed-Forward Networks]\n",
    "Either fc linear and conv with kernel size=1 can be used.\n",
    "TODO: In fact, kernel size can be extended. Find a proper kernel size in the data.\n",
    "  - Input= (B, S, D)\n",
    "  - Inner_State =(B, S, d_ff)\n",
    "  - Output = (B, S, D)\n",
    "\"\"\"\n",
    "class Poswise_FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_ff = config.FF_innerlayer_dim\n",
    "        self.d_model = config.d_model\n",
    "        self.conv1 = nn.Conv1d(self.d_model, self.d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(self.d_ff, self.d_model, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = nn.ReLU()(self.conv1(x.transpose(1, 2)))\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        return nn.LayerNorm(self.d_model)(output + residual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Mask Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"[Attention mask modules]\"\"\"\n",
    "def get_attn_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    \n",
    "    # eq(zero) is PAD token\n",
    "    # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
    "    # batch_size x len_q x len_k\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "\n",
    "def get_subsequent_attn_mask(seq):\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_position, d_model):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    return torch.FloatTensor(sinusoid_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Blocks and Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"[Encoder Block]\"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.attention_layer = Multihead_SelfAttention(config)\n",
    "        self.feedforward_layer = Poswise_FeedForward(config)\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, inputs, attn_mask):\n",
    "        outputs, attention = self.attention_layer(inputs, inputs, inputs, attn_mask)\n",
    "        encoder_outputs = self.feedforward_layer(outputs)\n",
    "        return encoder_outputs, attention\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"[Decoder Block]\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.masked_attention_layer = Multihead_SelfAttention(config)\n",
    "        self.attention_layer = Multihead_SelfAttention(config)\n",
    "        self.feedforward_layer = Poswise_FeedForward(config)\n",
    "        \n",
    "    def forward(self, inputs, encoder_outputs, masked_attn_mask, attn_mask):\n",
    "        outputs, dec_self_attnetion = self.masked_attention_layer(inputs, inputs, inputs, masked_attn_mask)\n",
    "        outputs, dec_enc_attention = self.attention_layer(outputs, encoder_outputs, encoder_outputs, attn_mask)\n",
    "        decoder_outputs = self.feedforward_layer(outputs)\n",
    "        return decoder_outputs, dec_self_attnetion, dec_enc_attention\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, input_vocab_size, input_len):\n",
    "        super().__init__()\n",
    "        d_emb = config.emb_dimension\n",
    "        n_layers = config.encoder_layer_num\n",
    "        self.position_info = torch.tensor([i for i in range(100)])\n",
    "        self.input_emb = nn.Embedding(input_vocab_size, d_emb)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(input_len+1, d_emb),freeze=True)\n",
    "        self.layers = nn.ModuleList([EncoderBlock(config.d_model) for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self, inputs): # inputs = [B, S]\n",
    "        encoded_inputs = self.input_emb(inputs) + self.pos_emb(self.position_info)\n",
    "        attn_mask = get_attn_mask(inputs, inputs)\n",
    "        attns = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            outputs, attn = layer(encoded_inputs, attn_mask)\n",
    "            attns.append(attn)\n",
    "            \n",
    "        return outputs, attns\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config, target_vocab_size, target_len):\n",
    "        super().__init__()\n",
    "        d_emb = config.emb_dimension\n",
    "        n_layers = config.decoder_layer_num\n",
    "        self.position_info = torch.tensor([i for i in range(100)])\n",
    "        self.target_emb = nn.Embedding(target_vocab_size, d_emb)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(target_len+1, d_emb), freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderBlock(config) for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self, targets, enc_inputs, enc_outputs): # targets = [B, S]\n",
    "        encoded_targets = self.target_emb(targets) + self.pos_emb(self.position_info)\n",
    "        subsequent_attn_mask = get_subsequent_attn_mask(targets)\n",
    "        attn_mask = get_attn_mask(targets, targets)\n",
    "        \n",
    "        self_attn_mask = torch.gt((attn_mask + subsequent_attn_mask), 0)\n",
    "        combo_attn_mask = get_attn_mask(targets, enc_inputs)\n",
    "        self_attns, combo_attns = [], []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            outputs, self_attn, combo_attn = layer(encoded_targets, enc_outputs, self_attn_mask, combo_attn_mask)\n",
    "            self_attns.append(self_attn)\n",
    "            combo_attns.append(combo_attn)\n",
    "            \n",
    "        return outputs, self_attns, combo_attns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, source_size, target_size, source_len, target_len):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.encoder = Encoder(config, source_size, source_len)\n",
    "        self.decoder = Decoder(config, target_size, target_len)\n",
    "        self.flatten_layer = nn.Linear(self.d_model, target_size, bias=False)\n",
    "        \n",
    "    # encoder inputs = torch.tensor([B, S])\n",
    "    def forward(self, encoder_inputs, decoder_inputs):\n",
    "        encoder_outputs, encoder_attns = self.encoder(encoder_inputs)\n",
    "        decoder_outputs, decoder_self_attns, decoder_combo_attns = self.decoder(decoder_inputs, encoder_inputs, encoder_outputs)\n",
    "        prob_outputs = self.flatten_layer(decoder_outputs)\n",
    "        return prob_outputs, encoder_attns, decoder_self_attns, decoder_combo_attns\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Load & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        self.idx = 0\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if not word in self.w2i:\n",
    "            self.w2i[word] = self.idx\n",
    "            self.i2w[self.idx] = word\n",
    "            self.idx += 1\n",
    "        \n",
    "    def __call__(self, word):\n",
    "        if not word in self.w2i:\n",
    "            return self.w2i['<unk>']\n",
    "        return self.w2i[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.w2i)\n",
    "    \n",
    "def build_vocab(lang_name, text):\n",
    "    counter = Counter()\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "    corpus = []\n",
    "    max_len = 0\n",
    "    \n",
    "    for sentence in text:\n",
    "        words = sentence.split(' ')\n",
    "        for word in words:\n",
    "            vocab.add_word(word)\n",
    "        corpus.append(' '.join(words[:-1]))\n",
    "        if len(words) > max_len:\n",
    "            max_len = len(words)\n",
    "            \n",
    "    print(\"%s corpus: %d num of words and %d num of sentences\" %(lang_name, len(vocab), len(corpus)))\n",
    "    return corpus, vocab, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset ...\n",
      "Preprocess ...\n",
      "English corpus: 23176 num of words and 10000 num of sentences\n",
      "Deutsch corpus: 33238 num of words and 10000 num of sentences\n",
      "English max length: 100 \n",
      "Geramn max length: 100\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Load dataset ...\")\n",
    "text_en = open('data/translation/train/train.en', 'r').readlines()[:10000]\n",
    "text_de = open('data/translation/train/train.de', 'r').readlines()[:10000]\n",
    "\n",
    "# Preprocessing\n",
    "print(\"Preprocess ...\")\n",
    "en_corpus, en_vocab, en_len = build_vocab('English', text_en)\n",
    "de_corpus, de_vocab, de_len = build_vocab('Deutsch', text_de)\n",
    "print(\"English max length: %d \\nGeramn max length: %d\" %(en_len, de_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_batch(batch_size, corpus, vocab, max_len):\n",
    "    batch_num = int(len(corpus)/batch_size)\n",
    "    result = []\n",
    "    for i in range(batch_num):\n",
    "        batch = []\n",
    "        for sentence in corpus[i*batch_size:(i+1)*batch_size]:\n",
    "            words = sentence.split(' ')\n",
    "            temp = []\n",
    "            for word in words:\n",
    "                temp.append(vocab.w2i[word])\n",
    "            for i in range(max_len - len(words)):\n",
    "                temp.append(vocab.w2i['<pad>'])\n",
    "            batch.append(temp)\n",
    "        result.append(torch.tensor(batch))\n",
    "    return result\n",
    "\n",
    "en_inputs = make_batch(32, en_corpus, en_vocab, en_len)\n",
    "de_inputs = make_batch(32, de_corpus, de_vocab, de_len)\n",
    "dataset = list(zip(en_inputs, de_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/20] Step[100/312] loss= 4.047763\n",
      "Epoch[1/20] Step[200/312] loss= 2.162891\n",
      "Epoch[1/20] Step[300/312] loss= 2.265270\n",
      "Epoch[2/20] Step[100/312] loss= 2.347775\n",
      "Epoch[2/20] Step[200/312] loss= 1.877549\n",
      "Epoch[2/20] Step[300/312] loss= 2.063701\n",
      "Epoch[3/20] Step[100/312] loss= 2.171968\n",
      "Epoch[3/20] Step[200/312] loss= 1.744647\n",
      "Epoch[3/20] Step[300/312] loss= 1.927909\n",
      "Epoch[4/20] Step[100/312] loss= 2.039999\n",
      "Epoch[4/20] Step[200/312] loss= 1.650614\n",
      "Epoch[4/20] Step[300/312] loss= 1.840509\n",
      "Epoch[5/20] Step[100/312] loss= 1.952139\n",
      "Epoch[5/20] Step[200/312] loss= 1.581113\n",
      "Epoch[5/20] Step[300/312] loss= 1.772204\n",
      "Epoch[6/20] Step[100/312] loss= 1.881098\n",
      "Epoch[6/20] Step[200/312] loss= 1.524466\n",
      "Epoch[6/20] Step[300/312] loss= 1.716600\n",
      "Epoch[7/20] Step[100/312] loss= 1.822548\n",
      "Epoch[7/20] Step[200/312] loss= 1.477340\n",
      "Epoch[7/20] Step[300/312] loss= 1.670751\n",
      "Epoch[8/20] Step[100/312] loss= 1.774290\n",
      "Epoch[8/20] Step[200/312] loss= 1.438858\n",
      "Epoch[8/20] Step[300/312] loss= 1.633108\n"
     ]
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "total_dataset = len(dataset)\n",
    "source_size = len(en_vocab) +1\n",
    "target_size = len(de_vocab) +1\n",
    "input_len = en_len\n",
    "target_len = de_len\n",
    "model = Transformer(config, source_size, target_size, input_len, target_len)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    for i, (enc_inputs, dec_inputs) in enumerate(dataset):\n",
    "        outputs, enc_self_attns, dec_self_attns, dec_combo_attns = model(enc_inputs, dec_inputs)\n",
    "        outputs = outputs.view(-1, outputs.size(2))\n",
    "        loss = criterion(outputs, dec_inputs.contiguous().view(-1))\n",
    "        avg_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ((i+1)%100 == 0):\n",
    "            print('Epoch[%d/20]' %(epoch + 1), 'Step[%02d/%d]' %((i+1), total_dataset), 'loss=', '{:.6f}'.format(avg_loss/100))\n",
    "            avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "predict, _, _, _ = model(enc_inputs, dec_inputs)\n",
    "predict = predict.data.max(1, keepdim=True)[1]\n",
    "print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
